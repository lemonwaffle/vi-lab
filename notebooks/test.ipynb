{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datamodules.celeba import CelebaDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from src.models.celeba.text_modules import CelebaTextEncoder, CelebaTextDecoder, CelebaTextClassifier\n",
    "from src.models.celeba.img_modules import CelebaImgClassifier\n",
    "import torch\n",
    "from torch.distributions import OneHotCategorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CelebaTextClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(torch.randn(5, 256, 71))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 40])"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CelebaImgClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(torch.randn(5, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 40])"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[autoreload of pytorch_lightning.utilities.distributed failed: Traceback (most recent call last):\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
      "]\n",
      "[autoreload of pytorch_lightning.metrics.classification failed: Traceback (most recent call last):\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/pytorch_lightning/metrics/classification/__init__.py\", line 17, in <module>\n",
      "    from pytorch_lightning.metrics.classification.f_beta import FBeta, Fbeta, F1\n",
      "ImportError: cannot import name 'Fbeta' from 'pytorch_lightning.metrics.classification.f_beta' (/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/pytorch_lightning/metrics/classification/f_beta.py)\n",
      "]\n",
      "[autoreload of pytorch_lightning.metrics.functional failed: Traceback (most recent call last):\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/imp.py\", line 314, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/__init__.py\", line 15, in <module>\n",
      "    from pytorch_lightning.metrics.functional.classification import (\n",
      "ImportError: cannot import name 'f1_score' from 'pytorch_lightning.metrics.functional.classification' (/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/pytorch_lightning/metrics/functional/classification.py)\n",
      "]\n",
      "[autoreload of pytorch_lightning.core.lightning failed: Traceback (most recent call last):\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: to_onnx() requires a code object with 0 free vars, not 2\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "dataset = CelebaDataset(\"data\", partition=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, text_str, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([256, 71])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "text_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CelebaTextDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = decoder(torch.randn(5, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llh = OneHotCategorical(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 71])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "llh.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18176"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "256 * 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12288"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "3 * 64 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "llh.log_prob(text_str).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 71])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "llh.mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "llh.mean.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotCategorical(logits).sample(torch.Size([num_samples]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[autoreload of src.models.celeba failed: Traceback (most recent call last):\n  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n    superreload(m, reload, self.old_objects)\n  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n    module = reload(module)\n  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/imp.py\", line 314, in reload\n    return importlib.reload(module)\n  File \"/home/yizhe/miniconda3/envs/vi-lab/lib/python3.7/importlib/__init__.py\", line 169, in reload\n    _bootstrap._exec(spec, module)\n  File \"<frozen importlib._bootstrap>\", line 630, in _exec\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n  File \"/home/yizhe/code/vi-lab/src/models/celeba/__init__.py\", line 1, in <module>\n    from .img_modules import CelebaImgDecoder, CelebaImgEncoder\nImportError: cannot import name 'CelebaImgDecoder' from 'src.models.celeba.img_modules' (/home/yizhe/code/vi-lab/src/models/celeba/img_modules.py)\n]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/CelebA/alphabet.json\") as alphabet_file:\n",
    "    alphabet = str(\"\".join(json.load(alphabet_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19867"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.utils import load_yaml\n",
    "from src.experiments import (\n",
    "    HierPMVAE_v1_Experiment,\n",
    "    Fusion_MVAE_Experiment\n",
    ")\n",
    "from src.models.encoders_decoders.base import (\n",
    "    DeepSVHNEncoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/tmp/vaevae_6.yaml\"\n",
    "hparams = load_yaml(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: data/train_32x32.mat\n",
      "Using downloaded and verified file: data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "expt = Fusion_MVAE_Experiment(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2135835"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "sum(p.numel() for p in expt.model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2122875"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "sum(p.numel() for p in expt.model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/mnist_svhn/hier_pmvae_v1.yaml\"\n",
    "hparams = load_yaml(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: data/train_32x32.mat\n",
      "Using downloaded and verified file: data/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "expt = HierPMVAE_v1_Experiment(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5\n",
    "K = 3\n",
    "m_latent_dim = 4\n",
    "s_latent_dim = 16\n",
    "\n",
    "inputs = [\n",
    "    torch.randn(B, 1, 28, 28),\n",
    "    # None,\n",
    "    torch.randn(B, 3, 32, 32)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.vdvae.encoder_decoder import (\n",
    "    Encoder,\n",
    "    Decoder,\n",
    "    parse_layer_string\n",
    ")\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 32, 32, 3)\n",
    "x_target = torch.randn(5, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = encoder.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_z, stats = decoder.forward(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortion_per_pixel = decoder.out_net.nll(px_z, x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_per_pixel = torch.zeros_like(distortion_per_pixel)\n",
    "ndims = np.prod(x.shape[1:])\n",
    "for statdict in stats:\n",
    "    rate_per_pixel += statdict['kl'].sum(dim=(1, 2, 3))\n",
    "rate_per_pixel /= ndims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_z = decoder.forward_uncond(5, t=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = decoder.out_net.sample(px_z)"
   ]
  },
  {
   "source": [
    "# Test Methods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = model.encode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "latents['s'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9a51a039b2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-9a51a039b2d8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "[o.shape for o in latents['m']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist, svhn = model.decode(latents, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 28, 28]), torch.Size([5, 3, 32, 32]))"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "mnist.shape, svhn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist, svhn = model.sample(K, mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 28, 28]), torch.Size([3, 3, 32, 32]))"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "mnist.shape, svhn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist, svhn = model.cross_reconstruct(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 28, 28]), torch.Size([5, 3, 32, 32]))"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "mnist.shape, svhn.shape"
   ]
  },
  {
   "source": [
    "# Test Log Prob"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob, latent, context = model.log_q_z_x(inputs, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "latent['s'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[torch.Size([15, 4]), torch.Size([15, 4])]"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "[l.shape for l in latent['m']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "context['s'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[torch.Size([15, 24]), torch.Size([15, 24])]"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "[c.shape for c in context['m']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Number of input items must be equal to number of context items.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-37e0d5e8507f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_q_z_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/vi-lab/src/models/vaes/hier_pmvae.py\u001b[0m in \u001b[0;36mlog_q_z_x\u001b[0;34m(self, inputs, latent, context, num_samples)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# If inputs not specified (and latent and context specified instead)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_q_z_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mq_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/vi-lab/src/models/vaes/hier_pmvae.py\u001b[0m in \u001b[0;36m_log_q_z_x\u001b[0;34m(self, latent, context)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Compute s_posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mlog_q_z_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_posterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_latent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Compute m_posteriors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vi-lab/lib/python3.7/site-packages/nflows/distributions/base.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 raise ValueError(\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0;34m\"Number of input items must be equal to number of context items.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 )\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of input items must be equal to number of context items."
     ]
    }
   ],
   "source": [
    "log_prob = model.log_q_z_x(latent=latent, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "log_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = model.log_p_z(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "log_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5, 1, 28, 28])\ntorch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "log_prob = model.log_p_x_z(inputs, latent, [1.0, 1.0], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "log_prob.shape"
   ]
  },
  {
   "source": [
    "# Test Objectives"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = expt.obj(\n",
    "    model,\n",
    "    inputs,\n",
    "    [1.0, 1.0],\n",
    "    kl_multiplier=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "elbo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = expt.test_step({\"data\": inputs}, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}