{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597549726219",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distribution:\n",
    "    \"\"\"Abstract class for unnormalized distribution\"\"\"\n",
    "    \n",
    "    def log_density(self, x):\n",
    "        \"\"\"\n",
    "            Computes vectorized log of unnormalized log density\n",
    "            \n",
    "            x (torch tensor of shape BxD): B points at which we compute log density\n",
    "            returns (torch tensor of shape B): \\log \\hat{\\pi}(x) \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def grad_log_density(self, x):\n",
    "        \"\"\"\n",
    "            Computes vectorized gradient \\nabla_x \\log \\pi(x)\n",
    "            \n",
    "            x (torch tensor of shape BxD): point at which we compute \\nabla \\log \\pi\n",
    "            returns (torch.tensor of shape BxD): gradients of log density\n",
    "        \"\"\"\n",
    "        # FIXME why clone? (not detach?)\n",
    "        x = x.clone().requires_grad_()\n",
    "        logp = self.log_density(x)\n",
    "        logp.sum().backward()\n",
    "\n",
    "        return x.grad\n",
    "\n",
    "\n",
    "class Proposal:\n",
    "    \"\"\"Abstract class for proposal\"\"\"\n",
    "    \n",
    "    def sample(self, x):\n",
    "        \"\"\"\n",
    "            Computes vectorized sample from proposal q(x' | x)\n",
    "            \n",
    "            x (torch tensor of shape BxD): current point from which we propose\n",
    "            returns: (torch tensor of shape BxD) new points\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def log_density(self, x, x_prime):\n",
    "        \"\"\"\n",
    "            Computes vectorized log of unnormalized log density\n",
    "            \n",
    "            x (torch tensor of shape BxD): B points at which we compute log density\n",
    "            returns (torch tensor of shape B): \\log q(x' | x) \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class MCMC:\n",
    "    def __init__(self, distribution, proposal):\n",
    "        \"\"\"\n",
    "            Constructs MCMC sampler\n",
    "        \n",
    "            distribution (Distribution): distribution from which we sample\n",
    "            proposal (Proposal): MCMC proposal\n",
    "        \"\"\"\n",
    "        self.distribution = distribution\n",
    "        self.proposal = proposal\n",
    "    \n",
    "    def _step(self, x, reject=True):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x_prime = self.proposal.sample(x)\n",
    "        acceptance_prob = self.acceptance_prob(x_prime, x) if reject else torch.ones(batch_size)\n",
    "        \n",
    "        # Keep accepted samples\n",
    "        mask = torch.rand(batch_size) < acceptance_prob\n",
    "        x[mask] = x_prime[mask]\n",
    "\n",
    "        # Keep track of # rejected samples\n",
    "        self._rejected += (1 - mask).type(torch.float32)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def simulate(self, initial_point, n_steps, n_parallel=10):\n",
    "        \"\"\"\n",
    "            Run `n_parallel ` simulations for `n_steps` starting from `initial_point`\n",
    "            \n",
    "            initial_point (torch tensor of shape D): starting point for all chains\n",
    "            n_steps (int): number of samples in Markov chain\n",
    "            n_parallel (int): number of parallel chains\n",
    "            returns: dict(\n",
    "                points (torch tensor of shape n_parallel x n_steps x D): samples\n",
    "                n_rejected (numpy array of shape n_parallel): number of rejections for each chain\n",
    "                rejection_rate (float): mean rejection rate over all chains\n",
    "                means (torch tensor of shape n_parallel x n_steps x D): means[c, s] = mean(points[c, :s])\n",
    "                variances (torch tensor of shape n_parallel x n_steps x D): variances[c, s, d] = variance(points[c, :s, d])\n",
    "            )\n",
    "        \"\"\"\n",
    "        xs = []\n",
    "        x = initial_point.repeat(n_parallel, 1)\n",
    "        self._rejected = torch.zeros(n_parallel)\n",
    "\n",
    "        dim = initial_point.shape[0]\n",
    "        sums = np.zeros([n_parallel, dim])\n",
    "        squares_sum = np.zeros([n_parallel, dim])\n",
    "\n",
    "        # For each chain, for each dim, over all steps\n",
    "        means = []\n",
    "        variances = []\n",
    "\n",
    "        for i in range(n_steps):\n",
    "            x = self._step(x)\n",
    "            xs.append(x.numpy().copy())\n",
    "\n",
    "            sums += xs[-1]\n",
    "            squares_sum += xs[-1]**2\n",
    "\n",
    "            mean, squares_mean = sums / (i + 1), squares_sum / (i + 1)\n",
    "            means.append(mean.copy())\n",
    "            variances.append(squares_mean - mean**2)\n",
    "\n",
    "        xs = np.stack(xs, axis=1)\n",
    "        means = np.stack(means, axis=1)\n",
    "        variances = np.stack(variances, axis=1)\n",
    "\n",
    "        return dict(\n",
    "            points=xs,\n",
    "            n_rejected=self._rejected.numpy(),\n",
    "            rejection_rate=(self._rejected / n_steps).mean().item(),\n",
    "            means=means,\n",
    "            variances=variances\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}